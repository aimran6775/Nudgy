//
//  NudgyConversationManager.swift
//  Nudge
//
//  Phase 8: Orchestrates the full conversation flow.
//  Ties together LLM, tools, memory, and conversation store.
//  Handles the complete message lifecycle including tool call loops.
//

import Foundation
import SwiftData

#if canImport(FoundationModels)
import FoundationModels
#endif

// MARK: - Conversation Response

/// Full response from a conversation turn.
struct ConversationResponse {
    let text: String
    let sideEffects: [ToolExecutionResult.ToolSideEffect]
    let toolCallsMade: Int
}

// MARK: - NudgyConversationManager

/// Orchestrates the full conversation flow between user and Nudgy.
@MainActor @Observable
final class NudgyConversationManager {
    
    static let shared = NudgyConversationManager()
    
    // MARK: - State
    
    let conversationStore = ConversationStore()
    
    /// Whether a response is being generated.
    private(set) var isGenerating = false
    
    /// Current streaming partial text.
    private(set) var streamingText = ""
    
    /// In-flight generation task (for cancellation).
    private var generationTask: Task<Void, Never>?
    
    /// Whether the current conversation is in brain dump mode.
    /// Brain dump mode uses a specialized system prompt that instructs the LLM
    /// to actively extract actionable tasks from the user's free-form speech.
    private(set) var isBrainDumpMode = false
    
    /// Accumulated transcript of all user messages in this conversation (for end-of-conversation sweep).
    private(set) var fullTranscript: [String] = []
    
    private init() {}
    
    // MARK: - Brain Dump Mode
    
    /// Start a brain dump conversation â€” uses the specialized brain dump system prompt.
    func startBrainDumpConversation(modelContext: ModelContext) {
        // End any existing conversation
        if conversationStore.isActive {
            clearConversation()
        }
        
        isBrainDumpMode = true
        fullTranscript = []
        
        let memory = NudgyMemory.shared
        let memoryContext = NudgyConfig.Features.memoryEnabled ? memory.memoryContext() : ""
        
        let hour = Calendar.current.component(.hour, from: .now)
        let timeOfDay = switch hour {
        case 5..<12: "morning"
        case 12..<17: "afternoon"
        case 17..<21: "evening"
        default: "late night"
        }
        let timeContext = "It's \(timeOfDay), \(Date.now.formatted(.dateTime.weekday(.wide).month().day()))"
        
        // Build task context to avoid duplicates
        let repo = NudgeRepository(modelContext: modelContext)
        let activeTasks = repo.fetchActiveQueue()
        let taskContext = activeTasks.prefix(10).map { task in
            "- \(task.emoji ?? "doc.text.fill") \(task.content)"
        }.joined(separator: "\n")
        
        let systemPrompt = NudgyPersonality.brainDumpConversationPrompt(
            memoryContext: memoryContext,
            taskContext: taskContext,
            timeContext: timeContext
        )
        
        conversationStore.startConversation(systemPrompt: systemPrompt)
        
        print("ðŸ§ ðŸŽ™ï¸ Brain dump conversation started. Active tasks: \(activeTasks.count), Memory facts: \(NudgyMemory.shared.store.facts.count)")
        print("ðŸ§ ðŸŽ™ï¸ System prompt length: \(systemPrompt.count) chars")
    }
    
    // MARK: - Conversation Lifecycle
    
    /// Ensure a conversation is active with the full system prompt.
    func ensureConversationActive(taskContext: String = "") {
        guard !conversationStore.isActive else { return }
        
        let memory = NudgyMemory.shared
        let memoryContext = NudgyConfig.Features.memoryEnabled ? memory.memoryContext() : ""
        
        let hour = Calendar.current.component(.hour, from: .now)
        let timeOfDay = switch hour {
        case 5..<12: "morning"
        case 12..<17: "afternoon"
        case 17..<21: "evening"
        default: "late night"
        }
        let timeContext = "It's \(timeOfDay), \(Date.now.formatted(.dateTime.weekday(.wide).month().day()))"
        
        let systemPrompt = NudgyPersonality.systemPrompt(
            memoryContext: memoryContext,
            taskContext: taskContext,
            timeContext: timeContext
        )
        
        conversationStore.startConversation(systemPrompt: systemPrompt)
    }
    
    // MARK: - Send Message (Non-Streaming)
    
    /// Send a user message and get Nudgy's response.
    func send(
        _ userMessage: String,
        modelContext: ModelContext
    ) async -> ConversationResponse {
        ensureConversationActive()
        conversationStore.addUserMessage(userMessage)
        if isBrainDumpMode { fullTranscript.append(userMessage) }
        isGenerating = true
        
        defer { isGenerating = false }
        
        do {
            let toolExecutor = NudgyToolExecutor(modelContext: modelContext)
            var allSideEffects: [ToolExecutionResult.ToolSideEffect] = []
            var totalToolCalls = 0
            
            // In brain dump mode, force tool calling on first iteration
            let tools = isBrainDumpMode
                ? NudgyToolDefinitions.brainDumpTools
                : NudgyToolDefinitions.allTools
            
            // Tool call loop (max 3 iterations to prevent runaway)
            var iterations = 0
            while iterations < 3 {
                iterations += 1
                
                let response = try await NudgyLLMService.shared.chatCompletion(
                    messages: conversationStore.apiMessages(),
                    tools: tools,
                    toolChoice: iterations == 1 && isBrainDumpMode ? "required" : "auto"
                )
                
                if response.hasToolCalls {
                    // Record assistant message with tool calls
                    let toolCallRecords = response.toolCalls.map {
                        ToolCallRecord(id: $0.id, functionName: $0.functionName, arguments: $0.arguments)
                    }
                    conversationStore.addAssistantMessage(response.content, toolCalls: toolCallRecords)
                    
                    // Execute tool calls
                    let results = toolExecutor.executeAll(response.toolCalls)
                    totalToolCalls += results.count
                    
                    for result in results {
                        conversationStore.addToolMessage(result.result, toolCallId: result.toolCallId)
                        allSideEffects.append(contentsOf: result.sideEffects)
                        
                        // Track side effects from THIS batch only
                        for effect in result.sideEffects {
                            switch effect {
                            case .taskCreated: conversationStore.tasksCreatedCount += 1
                            case .taskCompleted: conversationStore.tasksCompletedCount += 1
                            default: break
                            }
                        }
                    }
                    
                    // Continue loop â€” LLM will generate final response with tool results
                    continue
                }
                
                // No tool calls â€” this is the final response
                conversationStore.addAssistantMessage(response.content)
                NudgyMemory.shared.recordInteraction()
                
                // Notify data changes if we modified tasks
                if !allSideEffects.isEmpty {
                    NotificationCenter.default.post(name: .nudgeDataChanged, object: nil)
                }
                
                return ConversationResponse(
                    text: response.content,
                    sideEffects: allSideEffects,
                    toolCallsMade: totalToolCalls
                )
            }
            
            // Fallback if loop exhausted
            return ConversationResponse(
                text: "Hmm, I got a bit tangled up there. Could you say that again? ðŸ§",
                sideEffects: allSideEffects,
                toolCallsMade: totalToolCalls
            )
            
        } catch is NudgyLLMError where NudgyLLMService.shared.isCircuitOpen {
            #if DEBUG
            print("âš ï¸ Circuit breaker open â€” skipping OpenAI, going to fallbacks")
            #endif
            
            // Try Apple Foundation Models before dumb fallback
            let appleFMResponse = await appleFMFallback(userMessage, modelContext: modelContext)
            if let appleFMResponse {
                conversationStore.addAssistantMessage(appleFMResponse.text)
                return appleFMResponse
            }
            
            // Last resort: direct action handler (keyword matching)
            let fallbackResponse = directActionFallback(userMessage, modelContext: modelContext)
            conversationStore.addAssistantMessage(fallbackResponse.text)
            return fallbackResponse
            
        } catch {
            #if DEBUG
            print("âš ï¸ NudgyConversation error: \(error)")
            #endif
            
            // Try Apple Foundation Models before dumb fallback
            let appleFMResponse = await appleFMFallback(userMessage, modelContext: modelContext)
            if let appleFMResponse {
                conversationStore.addAssistantMessage(appleFMResponse.text)
                return appleFMResponse
            }
            
            // Last resort: direct action handler (keyword matching)
            let fallbackResponse = directActionFallback(userMessage, modelContext: modelContext)
            conversationStore.addAssistantMessage(fallbackResponse.text)
            return fallbackResponse
        }
    }
    
    // MARK: - Send Message (Streaming)
    
    /// Send a user message with streaming response.
    /// Calls onPartial with each chunk of text as it arrives.
    func sendStreaming(
        _ userMessage: String,
        modelContext: ModelContext,
        onPartial: @escaping @MainActor (String) -> Void
    ) async -> ConversationResponse {
        // Cancel any in-flight generation
        generationTask?.cancel()
        
        ensureConversationActive()
        conversationStore.addUserMessage(userMessage)
        if isBrainDumpMode { fullTranscript.append(userMessage) }
        isGenerating = true
        streamingText = ""
        
        defer {
            isGenerating = false
            streamingText = ""
        }
        
        do {
            let toolExecutor = NudgyToolExecutor(modelContext: modelContext)
            var allSideEffects: [ToolExecutionResult.ToolSideEffect] = []
            var totalToolCalls = 0
            
            // In brain dump mode, use task-focused tools with "required" tool_choice
            // so the LLM is forced to create tasks from user input.
            // In normal mode, use "auto" so the LLM decides.
            let tools = isBrainDumpMode
                ? NudgyToolDefinitions.brainDumpTools
                : NudgyToolDefinitions.allTools
            let brainDumpToolChoice = "required"
            
            print("ðŸ§ ðŸ’¬ sendStreaming: brainDump=\(isBrainDumpMode), msg='\(userMessage.prefix(60))'")
            
            // Tool call loop â€” iterate up to 3 times to handle multi-step tool use
            var iterations = 0
            while iterations < 3 {
                iterations += 1
                
                // First pass (or subsequent): check for tool calls (non-streaming)
                let response = try await NudgyLLMService.shared.chatCompletion(
                    messages: conversationStore.apiMessages(),
                    tools: tools,
                    toolChoice: iterations == 1 && isBrainDumpMode ? brainDumpToolChoice : "auto"
                )
                
                print("ðŸ§ ðŸ’¬ iteration \(iterations): hasToolCalls=\(response.hasToolCalls), toolCount=\(response.toolCalls.count), content='\(response.content.prefix(60))'")
                
                if response.hasToolCalls {
                    // Execute tool calls (may be multiple â€” brain dump can produce several tasks per turn)
                    let toolCallRecords = response.toolCalls.map {
                        ToolCallRecord(id: $0.id, functionName: $0.functionName, arguments: $0.arguments)
                    }
                    conversationStore.addAssistantMessage(response.content, toolCalls: toolCallRecords)
                    
                    let results = toolExecutor.executeAll(response.toolCalls)
                    totalToolCalls += results.count
                    
                    for result in results {
                        print("ðŸ§ ðŸ”§ Tool result [\(result.toolCallId)]: \(result.result.prefix(80))")
                        conversationStore.addToolMessage(result.result, toolCallId: result.toolCallId)
                        allSideEffects.append(contentsOf: result.sideEffects)
                        
                        // Track side effects from THIS batch only
                        for effect in result.sideEffects {
                            switch effect {
                            case .taskCreated: conversationStore.tasksCreatedCount += 1
                            case .taskCompleted: conversationStore.tasksCompletedCount += 1
                            default: break
                            }
                        }
                    }
                    
                    // Continue loop â€” LLM will generate final response with tool results
                    continue
                }
                
                // No tool calls â€” this is the final text response
                // Stream it to the user
                if iterations == 1 && !response.content.isEmpty {
                    // We already have the response from non-streaming call, use it directly
                    conversationStore.addAssistantMessage(response.content)
                    onPartial(response.content)
                    streamingText = response.content
                } else {
                    // After tool calls, stream the final response with tool results context
                    let streamResponse = try await NudgyLLMService.shared.streamChatCompletion(
                        messages: conversationStore.apiMessages()
                    ) { [weak self] partial in
                        self?.streamingText = partial
                        onPartial(partial)
                    }
                    conversationStore.addAssistantMessage(streamResponse.content)
                }
                
                NudgyMemory.shared.recordInteraction()
                
                if !allSideEffects.isEmpty {
                    NotificationCenter.default.post(name: .nudgeDataChanged, object: nil)
                }
                
                let finalText = conversationStore.messages.last?.content ?? ""
                print("ðŸ§ ðŸ’¬ Final response (\(totalToolCalls) tools): '\(finalText.prefix(80))'")
                
                return ConversationResponse(
                    text: finalText,
                    sideEffects: allSideEffects,
                    toolCallsMade: totalToolCalls
                )
            }
            
            // Exhausted loop â€” stream a final response anyway
            let streamResponse = try await NudgyLLMService.shared.streamChatCompletion(
                messages: conversationStore.apiMessages()
            ) { [weak self] partial in
                self?.streamingText = partial
                onPartial(partial)
            }
            conversationStore.addAssistantMessage(streamResponse.content)
            NudgyMemory.shared.recordInteraction()
            
            if !allSideEffects.isEmpty {
                NotificationCenter.default.post(name: .nudgeDataChanged, object: nil)
            }
            
            return ConversationResponse(
                text: streamResponse.content,
                sideEffects: allSideEffects,
                toolCallsMade: totalToolCalls
            )
            
        } catch {
            #if DEBUG
            print("âš ï¸ NudgyConversation streaming error: \(error)")
            #endif
            
            // Try Apple Foundation Models before dumb fallback
            let appleFMResponse = await appleFMFallback(userMessage, modelContext: modelContext)
            if let appleFMResponse {
                conversationStore.addAssistantMessage(appleFMResponse.text)
                onPartial(appleFMResponse.text)
                return appleFMResponse
            }
            
            // Last resort: direct action handler (keyword matching)
            let fallbackResponse = directActionFallback(userMessage, modelContext: modelContext)
            conversationStore.addAssistantMessage(fallbackResponse.text)
            onPartial(fallbackResponse.text)
            return fallbackResponse
        }
    }
    
    // MARK: - One-Shot Generation (Greetings, Reactions)
    
    /// Generate a one-shot response (no conversation context needed).
    /// Falls back: OpenAI â†’ Apple FM â†’ nil.
    func generateOneShotResponse(prompt: String) async -> String? {
        let personality = """
        You are Nudgy, a small excitable penguin ADHD coach.
        \(NudgyPersonality.communicationStyle)
        \(NudgyPersonality.responseRules)
        """
        
        // Try OpenAI first
        do {
            return try await NudgyLLMService.shared.generate(
                systemPrompt: personality,
                userPrompt: prompt,
                temperature: NudgyConfig.OpenAI.conversationTemperature
            )
        } catch {
            #if DEBUG
            print("âš ï¸ NudgyConversation one-shot OpenAI error: \(error)")
            #endif
        }
        
        // Fallback: Apple Foundation Models
        #if canImport(FoundationModels)
        if #available(iOS 26.0, *) {
            do {
                guard SystemLanguageModel.default.isAvailable else { return nil }
                let session = LanguageModelSession(
                    instructions: NudgyPersonality.compactPrompt()
                )
                let response = try await session.respond(to: prompt)
                #if DEBUG
                print("ðŸŽ NudgyConversation one-shot Apple FM success")
                #endif
                return response.content
            } catch {
                #if DEBUG
                print("âš ï¸ NudgyConversation one-shot Apple FM error: \(error)")
                #endif
            }
        }
        #endif
        
        return nil
    }
    
    // MARK: - Session Management
    
    /// End the current conversation and save summary to memory.
    /// Uses AI to generate a proper summary when available.
    /// In brain dump mode, runs a final extraction sweep on the full transcript.
    func endConversation() {
        generationTask?.cancel()
        generationTask = nil
        
        let wasBrainDump = isBrainDumpMode
        let transcript = fullTranscript
        
        // If conversation had enough turns, generate an AI summary
        if conversationStore.needsSummarization {
            let prompt = conversationStore.summarizationPrompt()
            let turnCount = conversationStore.turnCount
            let created = conversationStore.tasksCreatedCount
            let completed = conversationStore.tasksCompletedCount
            
            // End the conversation first (clears messages)
            _ = conversationStore.endConversation()
            
            // Fire-and-forget AI summary
            Task {
                let summaryPrefix = wasBrainDump ? "Brain dump conversation" : "Chat"
                let aiSummary = await generateOneShotResponse(prompt: prompt)
                let summary = ConversationSummary(
                    summary: aiSummary ?? "\(summaryPrefix) with \(turnCount) turns, \(created) tasks created",
                    turnCount: turnCount,
                    tasksCreated: created,
                    tasksCompleted: completed,
                    mood: wasBrainDump ? "brain-dump" : nil
                )
                NudgyMemory.shared.saveConversationSummary(summary)
                #if DEBUG
                print("ðŸ§  Conversation summary saved: \(summary.summary.prefix(80))")
                #endif
            }
        } else {
            // Short conversation â€” save basic summary
            if let summary = conversationStore.endConversation() {
                NudgyMemory.shared.saveConversationSummary(summary)
            }
        }
        
        // Reset brain dump state
        isBrainDumpMode = false
        fullTranscript = []
        
        #if DEBUG
        if wasBrainDump {
            print("ðŸ§ ðŸŽ™ï¸ Brain dump conversation ended. Transcript segments: \(transcript.count)")
        }
        #endif
    }
    
    /// End the brain dump conversation and return a summary of tasks created.
    /// Returns the count of tasks created during the conversation.
    func endBrainDumpConversation() -> Int {
        let tasksCreated = conversationStore.tasksCreatedCount
        endConversation()
        return tasksCreated
    }
    
    /// Clear conversation without saving.
    func clearConversation() {
        generationTask?.cancel()
        generationTask = nil
        isBrainDumpMode = false
        fullTranscript = []
        conversationStore.clearConversation()
    }
    
    /// Cancel in-flight generation.
    func cancelGeneration() {
        generationTask?.cancel()
        generationTask = nil
        isGenerating = false
        streamingText = ""
    }
    
    // MARK: - Apple Foundation Models Fallback
    
    /// Try Apple Foundation Models when OpenAI is unavailable.
    /// Returns nil if Apple FM is also unavailable, so caller can fall back further.
    private func appleFMFallback(
        _ input: String,
        modelContext: ModelContext
    ) async -> ConversationResponse? {
        #if canImport(FoundationModels)
        if #available(iOS 26.0, *) {
            do {
                guard SystemLanguageModel.default.isAvailable else {
                    #if DEBUG
                    print("ðŸŽ Apple FM not available on this device")
                    #endif
                    return nil
                }
                
                // Build memory context for personality continuity
                let memoryContext = NudgyConfig.Features.memoryEnabled
                    ? NudgyMemory.shared.memoryContext()
                    : ""
                
                // Build task context from repo
                let repo = NudgeRepository(modelContext: modelContext)
                let activeTasks = repo.fetchActiveQueue()
                let taskContext = activeTasks.prefix(5).map { task in
                    "\(task.emoji ?? "doc.text.fill") \(task.content)"
                }.joined(separator: "\n")
                
                // Use Apple FM tools for task actions
                let (tools, pendingActions) = NudgyToolbox.conversationTools(from: modelContext)
                
                let session = LanguageModelSession(
                    tools: tools,
                    instructions: NudgyPersonality.compactPrompt(
                        memoryContext: memoryContext,
                        taskContext: taskContext
                    )
                )
                
                let response = try await session.respond(to: input)
                
                #if DEBUG
                print("ðŸŽ Apple FM fallback success: '\(response.content.prefix(80))'")
                #endif
                
                // Process any pending tool actions
                var sideEffects: [ToolExecutionResult.ToolSideEffect] = []
                let pendingList = await pendingActions.actions
                for action in pendingList {
                    switch action {
                    case .create(let content):
                        sideEffects.append(.taskCreated(content: content))
                    case .complete(let taskId):
                        sideEffects.append(.taskCompleted(content: taskId.uuidString))
                    case .snooze(let taskId):
                        sideEffects.append(.taskSnoozed(content: taskId.uuidString))
                    }
                }
                
                if !sideEffects.isEmpty {
                    NotificationCenter.default.post(name: .nudgeDataChanged, object: nil)
                }
                
                NudgyMemory.shared.recordInteraction()
                
                return ConversationResponse(
                    text: response.content,
                    sideEffects: sideEffects,
                    toolCallsMade: 0
                )
            } catch {
                #if DEBUG
                print("âš ï¸ Apple FM fallback error: \(error)")
                #endif
                return nil
            }
        }
        #endif
        return nil
    }
    
    // MARK: - Direct Action Fallback (No AI)
    
    /// When AI is unavailable, parse user intent directly.
    private func directActionFallback(
        _ input: String,
        modelContext: ModelContext
    ) -> ConversationResponse {
        let lower = input.lowercased().trimmingCharacters(in: .whitespacesAndNewlines)
        let repo = NudgeRepository(modelContext: modelContext)
        
        // Task creation
        let createPrefixes = [
            "add ", "create ", "new task ", "remind me to ", "remind me ",
            "i need to ", "i gotta ", "i have to ", "i should ", "save ",
            "note ", "remember to "
        ]
        
        for prefix in createPrefixes {
            if lower.hasPrefix(prefix) {
                let content = String(input.dropFirst(prefix.count))
                    .trimmingCharacters(in: .whitespacesAndNewlines)
                if !content.isEmpty {
                    _ = repo.createManual(content: content)
                    HapticService.shared.cardAppear()
                    NotificationCenter.default.post(name: .nudgeDataChanged, object: nil)
                    return ConversationResponse(
                        text: "*scribbles with flippers* Got it! Added \"\(content)\" to your nudges! ðŸ“",
                        sideEffects: [.taskCreated(content: content)],
                        toolCallsMade: 0
                    )
                }
            }
        }
        
        // Task lookup
        if lower.contains("how many") || lower.contains("my tasks") || lower.contains("what do i have") {
            let active = repo.fetchActiveQueue()
            if active.isEmpty {
                return ConversationResponse(
                    text: "Clean slate! Nothing on your plate right now ðŸ§Š",
                    sideEffects: [], toolCallsMade: 0
                )
            }
            return ConversationResponse(
                text: "You've got \(active.count) nudge\(active.count == 1 ? "" : "s") lined up! Top one: \(active.first?.emoji ?? "pin.fill") \(active.first?.content ?? "") ðŸ’ª",
                sideEffects: [], toolCallsMade: 0
            )
        }
        
        // Greetings
        if ["hi", "hey", "hello", "yo"].contains(lower) {
            return ConversationResponse(
                text: "*excited waddle* Hey hey! What's on your mind? ðŸ§",
                sideEffects: [], toolCallsMade: 0
            )
        }
        
        // Emotional support
        if lower.contains("tired") || lower.contains("overwhelm") || lower.contains("stressed") {
            return ConversationResponse(
                text: "Hey. Opening the app already counts â€” I mean it. One fish at a time ðŸ’™",
                sideEffects: [], toolCallsMade: 0
            )
        }
        
        // Default: treat as task
        if lower.count > 2 && lower.count < 200 {
            _ = repo.createManual(content: input.trimmingCharacters(in: .whitespacesAndNewlines))
            HapticService.shared.cardAppear()
            NotificationCenter.default.post(name: .nudgeDataChanged, object: nil)
            return ConversationResponse(
                text: "*scribbles furiously* Added that to your nudges! ðŸ“ðŸ§",
                sideEffects: [.taskCreated(content: input)],
                toolCallsMade: 0
            )
        }
        
        return ConversationResponse(
            text: "Tell me what's on your mind! I can add tasks, check your list, or just chat ðŸ§",
            sideEffects: [], toolCallsMade: 0
        )
    }
}
